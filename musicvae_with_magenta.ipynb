{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MusicVAE\n",
    "paper: https://arxiv.org/pdf/1803.05428.pdf  \n",
    "reference code: https://github.com/magenta/magenta/tree/master/magenta/models/music_vae   \n",
    "datasets: https://magenta.tensorflow.org/datasets/groove\n",
    "\n",
    "Bidirectional Encoder + Hierarchical Decoder(conductor + decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Environment\n",
    "The recommended environment is docker.  \n",
    "If you are not familiar with docker environment, you can run this code locally in conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ONLY WHEN RUNNING LOCALLY ####\n",
    "# If you run this notebook locally(not in a docker container), you are better to use conda environment\n",
    "# Before you start to run this notebook, please install requirements pacakges and magenta\n",
    "\n",
    "# conda create --name magenta python=3.7\n",
    "# conda acitvate magneta\n",
    "\n",
    "# pip install -r ./requirements.txt\n",
    "# pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Datasets\n",
    "### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets locally\n",
    "# ! wget https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip datasets\n",
    "# ! unzip groove-v1.0.0-midionly.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./groove/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drummer</th>\n",
       "      <th>session</th>\n",
       "      <th>id</th>\n",
       "      <th>style</th>\n",
       "      <th>bpm</th>\n",
       "      <th>beat_type</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>midi_filename</th>\n",
       "      <th>audio_filename</th>\n",
       "      <th>duration</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/1</td>\n",
       "      <td>funk/groove1</td>\n",
       "      <td>138</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
       "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
       "      <td>27.872308</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/10</td>\n",
       "      <td>soul/groove10</td>\n",
       "      <td>102</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
       "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
       "      <td>37.691158</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/2</td>\n",
       "      <td>funk/groove2</td>\n",
       "      <td>105</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
       "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
       "      <td>36.351218</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/3</td>\n",
       "      <td>soul/groove3</td>\n",
       "      <td>86</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n",
       "      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n",
       "      <td>44.716543</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/4</td>\n",
       "      <td>soul/groove4</td>\n",
       "      <td>80</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n",
       "      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n",
       "      <td>47.987500</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    drummer                session                        id          style  \\\n",
       "0  drummer1  drummer1/eval_session   drummer1/eval_session/1   funk/groove1   \n",
       "1  drummer1  drummer1/eval_session  drummer1/eval_session/10  soul/groove10   \n",
       "2  drummer1  drummer1/eval_session   drummer1/eval_session/2   funk/groove2   \n",
       "3  drummer1  drummer1/eval_session   drummer1/eval_session/3   soul/groove3   \n",
       "4  drummer1  drummer1/eval_session   drummer1/eval_session/4   soul/groove4   \n",
       "\n",
       "   bpm beat_type time_signature  \\\n",
       "0  138      beat            4-4   \n",
       "1  102      beat            4-4   \n",
       "2  105      beat            4-4   \n",
       "3   86      beat            4-4   \n",
       "4   80      beat            4-4   \n",
       "\n",
       "                                       midi_filename  \\\n",
       "0  drummer1/eval_session/1_funk-groove1_138_beat_...   \n",
       "1  drummer1/eval_session/10_soul-groove10_102_bea...   \n",
       "2  drummer1/eval_session/2_funk-groove2_105_beat_...   \n",
       "3  drummer1/eval_session/3_soul-groove3_86_beat_4...   \n",
       "4  drummer1/eval_session/4_soul-groove4_80_beat_4...   \n",
       "\n",
       "                                      audio_filename   duration split  \n",
       "0  drummer1/eval_session/1_funk-groove1_138_beat_...  27.872308  test  \n",
       "1  drummer1/eval_session/10_soul-groove10_102_bea...  37.691158  test  \n",
       "2  drummer1/eval_session/2_funk-groove2_105_beat_...  36.351218  test  \n",
       "3  drummer1/eval_session/3_soul-groove3_86_beat_4...  44.716543  test  \n",
       "4  drummer1/eval_session/4_soul-groove4_80_beat_4...  47.987500  test  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir / 'info.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1150 entries, 0 to 1149\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   drummer         1150 non-null   object \n",
      " 1   session         1150 non-null   object \n",
      " 2   id              1150 non-null   object \n",
      " 3   style           1150 non-null   object \n",
      " 4   bpm             1150 non-null   int64  \n",
      " 5   beat_type       1150 non-null   object \n",
      " 6   time_signature  1150 non-null   object \n",
      " 7   midi_filename   1150 non-null   object \n",
      " 8   audio_filename  1090 non-null   object \n",
      " 9   duration        1150 non-null   float64\n",
      " 10  split           1150 non-null   object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 99.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess datasets with magenta\n",
    "Magneta provides convert midi data to note sequences.  \n",
    "We use this script to convert groove midi datasets to note sequences.  \n",
    "sciprt: magenta/magenta/scripts/convert_dir_to_note_sequences.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note sequences look like\n",
    "```\n",
    "ticks_per_quarter: 480\n",
    "time_signatures {\n",
    "  numerator: 4\n",
    "  denominator: 4\n",
    "}\n",
    "key_signatures {\n",
    "}\n",
    "tempos {\n",
    "  qpm: 95.00014250021376\n",
    "}\n",
    "notes {\n",
    "  pitch: 44\n",
    "  velocity: 30\n",
    "  start_time: 1.9473654999999999\n",
    "  end_time: 2.0486811374999996\n",
    "  is_drum: true\n",
    "}\n",
    "notes {\n",
    "  pitch: 38\n",
    "  velocity: 27\n",
    "  start_time: 2.4434173875\n",
    "  end_time: 2.4934173124999996\n",
    "  is_drum: true\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before run script we need to split train and test data\n",
    "train_dir = data_dir / \"train\"\n",
    "val_dir = data_dir / \"validation\"\n",
    "test_dir = data_dir / \"test\"\n",
    "\n",
    "if not train_dir.exists():\n",
    "    Path.mkdir(train_dir)\n",
    "if not val_dir.exists():\n",
    "    Path.mkdir(val_dir)\n",
    "if not test_dir.exists():\n",
    "    Path.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path_list = df[df[\"split\"] == \"train\"][\"midi_filename\"].to_list()\n",
    "val_data_path_list = df[df[\"split\"] == \"validation\"][\"midi_filename\"].to_list()\n",
    "test_data_path_list = df[df[\"split\"] == \"test\"][\"midi_filename\"].to_list()\n",
    "\n",
    "print(len(train_data_path_list), len(val_data_path_list), len(test_data_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some overlapped names, so we add prefix to file name\n",
    "for train_data_path in train_data_path_list:\n",
    "    prefix = Path(train_data_path).parents[1]\n",
    "    shutil.copy(data_dir / train_data_path, train_dir/f\"{prefix}_{Path(train_data_path).name}\")\n",
    "for val_data_path in val_data_path_list:\n",
    "    prefix = Path(val_data_path).parents[1]\n",
    "    shutil.copy(data_dir / val_data_path, val_dir/f\"{prefix}_{Path(val_data_path).name}\")\n",
    "for test_data_path in test_data_path_list:\n",
    "    prefix = Path(test_data_path).parents[1]\n",
    "    shutil.copy(data_dir / test_data_path, test_dir/f\"{prefix}_{Path(test_data_path).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(train_dir.glob(\"*.mid\"))), len(list(val_dir.glob(\"*.mid\"))), len(list(test_dir.glob(\"*.mid\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir preprocessed_datasets\n",
    "! python magenta/scripts/convert_dir_to_note_sequences.py --input_dir=./groove/train --output_file=./preprocessed_datasets/groove_train.tfrecord --recursive=True --log=INFO\n",
    "! python magenta/scripts/convert_dir_to_note_sequences.py --input_dir=./groove/val --output_file=./preprocessed_datasets/groove_validation.tfrecord --recursive=True --log=INFO\n",
    "! python magenta/scripts/convert_dir_to_note_sequences.py --input_dir=./groove/test --output_file=./preprocessed_datasets/groove_test.tfrecord --recursive=True --log=INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MusicVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make MusicVAE config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magenta\n",
    "\n",
    "import collections\n",
    "\n",
    "from magenta.models.music_vae.configs import Config, HParams, CONFIG_MAP\n",
    "\n",
    "from magenta.common import merge_hparams\n",
    "from magenta.contrib import training as contrib_training\n",
    "from magenta.models.music_vae import data\n",
    "from magenta.models.music_vae import data_hierarchical\n",
    "from magenta.models.music_vae import lstm_models\n",
    "from magenta.models.music_vae.base_model import MusicVAE\n",
    "\n",
    "import note_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters in paper\n",
    "\n",
    "NUM_FEATURES = 2 ** 9\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "ENCODER_HIDDEN_SIZE = 2048\n",
    "LATENT_DIM = 512\n",
    "NUM_SEGMENTS = 4\n",
    "CONDUCTOR_HIDDEN_SIZE = 1024\n",
    "CONDUCTOR_OUTPUT_SIZE = 512\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DECODER_HIDDEN_SIZE = 1024\n",
    "DECODER_OUTPUT_SIZE = NUM_FEATURES\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "NUM_BARS = 4\n",
    "MAX_SEQ_LENGTH = NUM_BARS * NUM_SEGMENTS\n",
    "BATCH_SIZE = 512\n",
    "MAX_BETA = 0.2 # lower beta means better reconstrucction\n",
    "FREE_BITS = 48\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MAP['musicvae_groove_4bar'] = Config(\n",
    "    model=MusicVAE(\n",
    "        lstm_models.BidirectionalLstmEncoder(),\n",
    "        lstm_models.HierarchicalLstmDecoder(\n",
    "            lstm_models.CategoricalLstmDecoder(),\n",
    "            level_lengths=[4, 4])\n",
    "        ),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_seq_len=MAX_SEQ_LENGTH,  \n",
    "            z_size=LATENT_DIM,\n",
    "            enc_rnn_size=[ENCODER_HIDDEN_SIZE],\n",
    "            dec_rnn_size=[DECODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE],\n",
    "            max_beta=MAX_BETA,\n",
    "            free_bits=FREE_BITS,\n",
    "            dropout_keep_prob=0.3,\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.DrumsConverter(\n",
    "        max_bars=100,  # Truncate long drum sequences before slicing.\n",
    "        slice_bars=4,\n",
    "        steps_per_quarter=4,\n",
    "        roll_input=True),\n",
    "    train_examples_path=\"./preprocessed_datasets/groove_train.tfrecord\",  # the train tfrecord file path\n",
    "    eval_examples_path=\"./preprocessed_datasets/groove_validation.tfrecord\" # the eval tfrecord file path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MusicVAE\n",
    "\n",
    "We use the script(https://github.com/magenta/magenta/blob/main/magenta/models/music_vae/music_vae_train.py) to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magenta.models.music_vae.music_vae_train import main, flags, FLAGS, train, evaluate\n",
    "from magenta.models.music_vae import configs\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set arguments for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from argparse import ArgumentParser\n",
    "\n",
    "# parser = ArgumentParser(description=\"Argument for training\")\n",
    "# parser.add_argument(\"examples_path\", type=str, default=\"./preprocessed_datasets/groove.tfrecord\", help=\"Path to a TFRecord file of NoteSequence examples. Overrides the config.\")\n",
    "# parser.add_argument(\"tfds_name\", type=str, default='', help=\"TensorFlow Datasets dataset name to use. Overrides the config.\")\n",
    "# parser.add_argument(\"run_dir\", type=str, default=\"./outputs/\", help=\"Path where checkpoints and summary events will be located during training and evaluation. Separate subdirectories 'train' and 'eval' will be created within this directory.\")\n",
    "# parser.add_argument(\"num_steps\", type=int, default=50000, help=\"Number of training steps or `None` for infinite.\")\n",
    "# parser.add_argument(\"eval_num_batches\", type=int, default=2000, help=\"Number of batches to use during evaluation or 'None' for all batches in the data source.\")\n",
    "# parser.add_argument(\"checkpoints_to_keep\", type=int, default=25, help=\"Maximum number of checkpoints to keep in 'train' mode or 0 for infinite.\")\n",
    "# parser.add_argument(\"mode\", type=str, default=\"train\", help=\"train or eval\")\n",
    "# parser.add_argument(\"log\", type=str, default=\"INFO\", help=\"DEBUG, INFO, WARN, ERROR, or FATAL.\")\n",
    "\n",
    "args = {\n",
    "    \"master\": '',\n",
    "    \"examples_path\": \"\", # we have already set examples path in config\n",
    "    \"tfds_name\": \"\",\n",
    "    \"run_dir\": \"./outputs/\",\n",
    "    \"num_steps\": 500000,\n",
    "    \"eval_num_batches\": 2000,\n",
    "    \"checkpoints_to_keep\": 25,\n",
    "    \"keep_checkpoint_every_n_hours\": 1,\n",
    "    \"mode\": \"train\",\n",
    "    \"log\": \"INFO\",\n",
    "    \"hparams\": '',\n",
    "    \"cache_dataset\": True,\n",
    "    \"task\": 0,\n",
    "    \"num_ps_tasks\": 0,\n",
    "    'num_sync_workers': 0,\n",
    "    'eval_dir_suffix': '',\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(args[\"log\"])\n",
    "\n",
    "def run(config_map, args, tf_file_reader=tf.data.TFRecordDataset, file_reader=tf.python_io.tf_record_iterator):\n",
    "    if not args[\"run_dir\"]:\n",
    "        raise ValueError('Invalid run directory: %s' % args[\"run_dir\"])\n",
    "    run_dir = os.path.expanduser(args[\"run_dir\"])\n",
    "    train_dir = os.path.join(run_dir, 'train')\n",
    "    \n",
    "    if args[\"mode\"] not in ['train', 'eval']:\n",
    "      raise ValueError('Invalid mode: %s' % args[\"mode\"])\n",
    "\n",
    "    config = config_map\n",
    "    if args[\"hparams\"]:\n",
    "      config.hparams.parse(args[\"hparams\"])\n",
    "    config_update_map = {}\n",
    "    if args[\"examples_path\"]:\n",
    "      config_update_map['%s_examples_path' % args[\"mode\"]] = os.path.expanduser(args[\"examples_path\"])\n",
    "    if args[\"tfds_name\"]:\n",
    "      if args[\"examples_path\"]:\n",
    "        raise ValueError(\n",
    "            'At most one of --examples_path and --tfds_name can be set.')\n",
    "      config_update_map['tfds_name'] = args[\"tfds_name\"]\n",
    "      config_update_map['eval_examples_path'] = None\n",
    "      config_update_map['train_examples_path'] = None\n",
    "    config = configs.update_config(config, config_update_map)\n",
    "\n",
    "    if args[\"mode\"] == 'train':\n",
    "      is_training = True\n",
    "    elif args[\"mode\"] == 'eval':\n",
    "      is_training = False\n",
    "    else:\n",
    "      raise ValueError('Invalid mode: {}'.format(args[\"mode\"]))\n",
    "\n",
    "    def dataset_fn():\n",
    "      return data.get_dataset(\n",
    "          config,\n",
    "          tf_file_reader=tf_file_reader,\n",
    "          is_training=is_training,\n",
    "          cache_dataset=args[\"cache_dataset\"])\n",
    "\n",
    "    if is_training:\n",
    "      train(\n",
    "          train_dir,\n",
    "          config=config,\n",
    "          dataset_fn=dataset_fn,\n",
    "          checkpoints_to_keep=args[\"checkpoints_to_keep\"],\n",
    "          keep_checkpoint_every_n_hours=args[\"keep_checkpoint_every_n_hours\"],\n",
    "          num_steps=args[\"num_steps\"],\n",
    "          master=args[\"master\"],\n",
    "          num_sync_workers=args[\"num_sync_workers\"],\n",
    "          num_ps_tasks=args[\"num_ps_tasks\"],\n",
    "          task=args[\"task\"])\n",
    "    else:\n",
    "      num_batches = args[\"eval_num_batches\"] or data.count_examples(\n",
    "          config.eval_examples_path,\n",
    "          config.tfds_name,\n",
    "          config.data_converter,\n",
    "          file_reader) // config.hparams.batch_size\n",
    "      eval_dir = os.path.join(run_dir, 'eval' + args[\"eval_dir_suffix\"])\n",
    "      evaluate(\n",
    "          train_dir,\n",
    "          eval_dir,\n",
    "          config=config,\n",
    "          dataset_fn=dataset_fn,\n",
    "          num_batches=num_batches,\n",
    "          master=args[\"master\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(CONFIG_MAP['musicvae_groove_4bar'], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples\n",
    "Magenta also provides scripts to generate samples from trained models  \n",
    "scripts: magenta/magenta/models/music_vae/music_vae_generate.py\n",
    "\n",
    "pretrained model link: https://drive.google.com/file/d/1ALoQpdyUI5oHJCqe92Cb_oimv1CXP3cN/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "import sys\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your checkpoint path. It will be .tar file including .index and .data\n",
    "checkpoint_path = Path('./outputs/model.ckpt-50000.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"run_dir\": None,\n",
    "    \"checkpoint_file\": checkpoint_path,\n",
    "    \"output_dir\": \"./outputs/generation_samples/\",\n",
    "    \"mode\": \"sample\",\n",
    "    \"input_midi_1\": None,\n",
    "    \"input_midi_2\": None,\n",
    "    \"num_outputs\": 5,\n",
    "    \"max_batch_size\": 8,\n",
    "    \"temperature\": 0.5,\n",
    "    \"log\": \"INFO\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior()\n",
    "logging = tf.logging\n",
    "logging.set_verbosity(generation_args[\"log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_run(config_map, generation_args):\n",
    "  \"\"\"Load model params, save config file and start trainer.\n",
    "  Args:\n",
    "    config_map: Dictionary mapping configuration name to Config object.\n",
    "  Raises:\n",
    "    ValueError: if required flags are missing or invalid.\n",
    "  \"\"\"\n",
    "  date_and_time = time.strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "  if generation_args[\"run_dir\"] is None == generation_args[\"checkpoint_file\"] is None:\n",
    "    raise ValueError(\n",
    "        'Exactly one of `--run_dir` or `--checkpoint_file` must be specified.')\n",
    "  if generation_args[\"output_dir\"] is None:\n",
    "    raise ValueError('`--output_dir` is required.')\n",
    "  tf.gfile.MakeDirs(generation_args[\"output_dir\"])\n",
    "  if generation_args[\"mode\"] != 'sample' and generation_args[\"mode\"] != 'interpolate':\n",
    "    raise ValueError('Invalid value for `--mode`: %s' % generation_args[\"mode\"])\n",
    "\n",
    "  config = config_map\n",
    "  config.data_converter.max_tensors_per_item = None\n",
    "\n",
    "  if generation_args[\"mode\"] == 'interpolate':\n",
    "    if generation_args[\"input_midi_1\"] is None or generation_args[\"input_midi_2\"] is None:\n",
    "      raise ValueError(\n",
    "          '`--input_midi_1` and `--input_midi_2` must be specified in '\n",
    "          '`interpolate` mode.')\n",
    "    input_midi_1 = os.path.expanduser(generation_args[\"input_midi_1\"])\n",
    "    input_midi_2 = os.path.expanduser(generation_args[\"input_midi_2\"])\n",
    "    if not os.path.exists(input_midi_1):\n",
    "      raise ValueError('Input MIDI 1 not found: %s' % generation_args[\"input_midi_1\"])\n",
    "    if not os.path.exists(input_midi_2):\n",
    "      raise ValueError('Input MIDI 2 not found: %s' % generation_args[\"input_midi_2\"])\n",
    "    input_1 = note_seq.midi_file_to_note_sequence(input_midi_1)\n",
    "    input_2 = note_seq.midi_file_to_note_sequence(input_midi_2)\n",
    "\n",
    "    def _check_extract_examples(input_ns, path, input_number):\n",
    "      \"\"\"Make sure each input returns exactly one example from the converter.\"\"\"\n",
    "      tensors = config.data_converter.to_tensors(input_ns).outputs\n",
    "      if not tensors:\n",
    "        print(\n",
    "            'MusicVAE configs have very specific input requirements. Could not '\n",
    "            'extract any valid inputs from `%s`. Try another MIDI file.' % path)\n",
    "        sys.exit()\n",
    "      elif len(tensors) > 1:\n",
    "        basename = os.path.join(\n",
    "            generation_args[\"output_dir\"],\n",
    "            '%s_input%d-extractions_%s-*-of-%03d.mid' %\n",
    "            (\"musicvae_groove_4bar\", input_number, date_and_time, len(tensors)))\n",
    "        for i, ns in enumerate(config.data_converter.from_tensors(tensors)):\n",
    "          note_seq.sequence_proto_to_midi_file(\n",
    "              ns, basename.replace('*', '%03d' % i))\n",
    "        print(\n",
    "            '%d valid inputs extracted from `%s`. Outputting these potential '\n",
    "            'inputs as `%s`. Call script again with one of these instead.' %\n",
    "            (len(tensors), path, basename))\n",
    "        sys.exit()\n",
    "    logging.info(\n",
    "        'Attempting to extract examples from input MIDIs using config `%s`...',\n",
    "        \"musicvae_groove_4bar\")\n",
    "    _check_extract_examples(input_1, generation_args[\"input_midi_1\"], 1)\n",
    "    _check_extract_examples(input_2, generation_args[\"input_midi_2\"], 2)\n",
    "\n",
    "  logging.info('Loading model...')\n",
    "  if generation_args[\"run_dir\"]:\n",
    "    checkpoint_dir_or_path = os.path.expanduser(\n",
    "        os.path.join(generation_args[\"run_dir\"], 'train'))\n",
    "  else:\n",
    "    checkpoint_dir_or_path = os.path.expanduser(generation_args[\"checkpoint_file\"])\n",
    "    print(checkpoint_dir_or_path)\n",
    "  model = TrainedModel(\n",
    "      config, batch_size=min(generation_args[\"max_batch_size\"], generation_args[\"num_outputs\"]),\n",
    "      checkpoint_dir_or_path=checkpoint_dir_or_path)\n",
    "\n",
    "  if generation_args[\"mode\"] == 'interpolate':\n",
    "    logging.info('Interpolating...')\n",
    "    _, mu, _ = model.encode([input_1, input_2])\n",
    "    z = np.array([\n",
    "        _slerp(mu[0], mu[1], t) for t in np.linspace(0, 1, generation_args[\"num_outputs\"])])\n",
    "    results = model.decode(\n",
    "        length=config.hparams.max_seq_len,\n",
    "        z=z,\n",
    "        temperature=generation_args[\"temperature\"])\n",
    "  elif generation_args[\"mode\"] == 'sample':\n",
    "    logging.info('Sampling...')\n",
    "    results = model.sample(\n",
    "        n=generation_args[\"num_outputs\"],\n",
    "        length=config.hparams.max_seq_len,\n",
    "        temperature=generation_args[\"temperature\"])\n",
    "\n",
    "  basename = os.path.join(\n",
    "      generation_args[\"output_dir\"],\n",
    "      '%s_%s_%s-*-of-%03d.mid' %\n",
    "      (\"musicvae_groove_4bar\", generation_args[\"mode\"], date_and_time, generation_args[\"num_outputs\"]))\n",
    "  logging.info('Outputting %d files as `%s`...', generation_args[\"num_outputs\"], basename)\n",
    "  for i, ns in enumerate(results):\n",
    "    note_seq.sequence_proto_to_midi_file(ns, basename.replace('*', '%03d' % i))\n",
    "\n",
    "  logging.info('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_run(CONFIG_MAP[\"musicvae_groove_4bar\"], generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('magenta')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0746120117e608c640370de5789262e88f3f302f4d84eea41e4e29e486cf0a25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
