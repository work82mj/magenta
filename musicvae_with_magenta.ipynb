{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MusicVAE\n",
    "paper: https://arxiv.org/pdf/1803.05428.pdf  \n",
    "reference code: https://github.com/magenta/magenta/tree/master/magenta/models/music_vae   \n",
    "datasets: https://magenta.tensorflow.org/datasets/groove\n",
    "\n",
    "Bidirectional Encoder + Hierarchical Decoder(conductor + decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Environment\n",
    "The recommended environment is docker.  \n",
    "If you are not familiar with docker environment, you can run this code locally in conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ONLY WHEN RUNNING LOCALLY ####\n",
    "# If you run this notebook locally(not in a docker container), you are better to use conda environment\n",
    "# Before you start to run this notebook, please install requirements pacakges and magenta\n",
    "\n",
    "# conda create --name magenta python=3.7\n",
    "# conda acitvate magneta\n",
    "\n",
    "# pip install -r ./requirements.txt\n",
    "# cd magenta && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Datasets\n",
    "### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets locally\n",
    "# ! wget https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip datasets\n",
    "# ! unzip groove-v1.0.0-midionly.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./groove/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drummer</th>\n",
       "      <th>session</th>\n",
       "      <th>id</th>\n",
       "      <th>style</th>\n",
       "      <th>bpm</th>\n",
       "      <th>beat_type</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>midi_filename</th>\n",
       "      <th>audio_filename</th>\n",
       "      <th>duration</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/1</td>\n",
       "      <td>funk/groove1</td>\n",
       "      <td>138</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
       "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
       "      <td>27.872308</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/10</td>\n",
       "      <td>soul/groove10</td>\n",
       "      <td>102</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
       "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
       "      <td>37.691158</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/2</td>\n",
       "      <td>funk/groove2</td>\n",
       "      <td>105</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
       "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
       "      <td>36.351218</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/3</td>\n",
       "      <td>soul/groove3</td>\n",
       "      <td>86</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n",
       "      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n",
       "      <td>44.716543</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/4</td>\n",
       "      <td>soul/groove4</td>\n",
       "      <td>80</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n",
       "      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n",
       "      <td>47.987500</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    drummer                session                        id          style  \\\n",
       "0  drummer1  drummer1/eval_session   drummer1/eval_session/1   funk/groove1   \n",
       "1  drummer1  drummer1/eval_session  drummer1/eval_session/10  soul/groove10   \n",
       "2  drummer1  drummer1/eval_session   drummer1/eval_session/2   funk/groove2   \n",
       "3  drummer1  drummer1/eval_session   drummer1/eval_session/3   soul/groove3   \n",
       "4  drummer1  drummer1/eval_session   drummer1/eval_session/4   soul/groove4   \n",
       "\n",
       "   bpm beat_type time_signature  \\\n",
       "0  138      beat            4-4   \n",
       "1  102      beat            4-4   \n",
       "2  105      beat            4-4   \n",
       "3   86      beat            4-4   \n",
       "4   80      beat            4-4   \n",
       "\n",
       "                                       midi_filename  \\\n",
       "0  drummer1/eval_session/1_funk-groove1_138_beat_...   \n",
       "1  drummer1/eval_session/10_soul-groove10_102_bea...   \n",
       "2  drummer1/eval_session/2_funk-groove2_105_beat_...   \n",
       "3  drummer1/eval_session/3_soul-groove3_86_beat_4...   \n",
       "4  drummer1/eval_session/4_soul-groove4_80_beat_4...   \n",
       "\n",
       "                                      audio_filename   duration split  \n",
       "0  drummer1/eval_session/1_funk-groove1_138_beat_...  27.872308  test  \n",
       "1  drummer1/eval_session/10_soul-groove10_102_bea...  37.691158  test  \n",
       "2  drummer1/eval_session/2_funk-groove2_105_beat_...  36.351218  test  \n",
       "3  drummer1/eval_session/3_soul-groove3_86_beat_4...  44.716543  test  \n",
       "4  drummer1/eval_session/4_soul-groove4_80_beat_4...  47.987500  test  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir / 'info.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1150 entries, 0 to 1149\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   drummer         1150 non-null   object \n",
      " 1   session         1150 non-null   object \n",
      " 2   id              1150 non-null   object \n",
      " 3   style           1150 non-null   object \n",
      " 4   bpm             1150 non-null   int64  \n",
      " 5   beat_type       1150 non-null   object \n",
      " 6   time_signature  1150 non-null   object \n",
      " 7   midi_filename   1150 non-null   object \n",
      " 8   audio_filename  1090 non-null   object \n",
      " 9   duration        1150 non-null   float64\n",
      " 10  split           1150 non-null   object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 99.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess datasets with magenta\n",
    "Magneta provides convert midi data to note sequences.  \n",
    "We use this script to convert groove midi datasets to note sequences.  \n",
    "sciprt: magenta/magenta/scripts/convert_dir_to_note_sequences.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note sequences look like\n",
    "```\n",
    "ticks_per_quarter: 480\n",
    "time_signatures {\n",
    "  numerator: 4\n",
    "  denominator: 4\n",
    "}\n",
    "key_signatures {\n",
    "}\n",
    "tempos {\n",
    "  qpm: 95.00014250021376\n",
    "}\n",
    "notes {\n",
    "  pitch: 44\n",
    "  velocity: 30\n",
    "  start_time: 1.9473654999999999\n",
    "  end_time: 2.0486811374999996\n",
    "  is_drum: true\n",
    "}\n",
    "notes {\n",
    "  pitch: 38\n",
    "  velocity: 27\n",
    "  start_time: 2.4434173875\n",
    "  end_time: 2.4934173124999996\n",
    "  is_drum: true\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before run script we need to split train and test data\n",
    "train_dir = data_dir / \"train\"\n",
    "val_dir = data_dir / \"validation\"\n",
    "test_dir = data_dir / \"test\"\n",
    "\n",
    "if not train_dir.exists():\n",
    "    Path.mkdir(train_dir)\n",
    "if not val_dir.exists():\n",
    "    Path.mkdir(val_dir)\n",
    "if not test_dir.exists():\n",
    "    Path.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897 124 129\n"
     ]
    }
   ],
   "source": [
    "train_data_path_list = df[df[\"split\"] == \"train\"][\"midi_filename\"].to_list()\n",
    "val_data_path_list = df[df[\"split\"] == \"validation\"][\"midi_filename\"].to_list()\n",
    "test_data_path_list = df[df[\"split\"] == \"test\"][\"midi_filename\"].to_list()\n",
    "\n",
    "print(len(train_data_path_list), len(val_data_path_list), len(test_data_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some overlapped names, so we add prefix to file name\n",
    "for train_data_path in train_data_path_list:\n",
    "    prefix = Path(train_data_path).parents[1]\n",
    "    shutil.copy(data_dir / train_data_path, train_dir/f\"{prefix}_{Path(train_data_path).name}\")\n",
    "for val_data_path in val_data_path_list:\n",
    "    prefix = Path(val_data_path).parents[1]\n",
    "    shutil.copy(data_dir / val_data_path, val_dir/f\"{prefix}_{Path(val_data_path).name}\")\n",
    "for test_data_path in test_data_path_list:\n",
    "    prefix = Path(test_data_path).parents[1]\n",
    "    shutil.copy(data_dir / test_data_path, test_dir/f\"{prefix}_{Path(test_data_path).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897 124 129\n"
     ]
    }
   ],
   "source": [
    "print(len(list(train_dir.glob(\"*.mid\"))), len(list(val_dir.glob(\"*.mid\"))), len(list(test_dir.glob(\"*.mid\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: preprocessed_datasets: File exists\n",
      "/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"magenta/scripts/convert_dir_to_note_sequences.py\", line 29, in <module>\n",
      "    from note_seq import abc_parser\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/note_seq/__init__.py\", line 20, in <module>\n",
      "    import note_seq.audio_io\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/note_seq/audio_io.py\", line 21, in <module>\n",
      "    import librosa\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/librosa/__init__.py\", line 12, in <module>\n",
      "    from . import core\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/librosa/core/__init__.py\", line 126, in <module>\n",
      "    from .audio import *  # pylint: disable=wildcard-import\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/librosa/core/audio.py\", line 14, in <module>\n",
      "    import resampy\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/resampy/__init__.py\", line 7, in <module>\n",
      "    from .core import *\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/resampy/core.py\", line 9, in <module>\n",
      "    from .interpn import resample_f_s, resample_f_p\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/resampy/interpn.py\", line 111, in <module>\n",
      "    nopython=True,\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/np/ufunc/decorators.py\", line 179, in wrap\n",
      "    guvec.add(fty)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/np/ufunc/ufuncbuilder.py\", line 212, in add\n",
      "    self.nb_func, targetoptions, sig)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/np/ufunc/ufuncbuilder.py\", line 143, in _compile_element_wise_function\n",
      "    cres = nb_func.compile(sig, **targetoptions)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/np/ufunc/ufuncbuilder.py\", line 92, in compile\n",
      "    return self._compile_core(sig, flags, locals)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/np/ufunc/ufuncbuilder.py\", line 127, in _compile_core\n",
      "    flags=flags, locals=locals)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler.py\", line 568, in compile_extra\n",
      "    return pipeline.compile_extra(func)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler.py\", line 339, in compile_extra\n",
      "    return self._compile_bytecode()\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler.py\", line 401, in _compile_bytecode\n",
      "    return self._compile_core()\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler.py\", line 372, in _compile_core\n",
      "    pm.run(self.state)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler_machinery.py\", line 332, in run\n",
      "    self._runPass(idx, pass_inst, state)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler_lock.py\", line 32, in _acquire_compile_lock\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler_machinery.py\", line 291, in _runPass\n",
      "    mutated |= check(pss.run_pass, internal_state)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler_machinery.py\", line 264, in check\n",
      "    mangled = func(compiler_state)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typed_passes.py\", line 98, in run_pass\n",
      "    raise_errors=self._raise_errors)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typed_passes.py\", line 70, in type_inference_stage\n",
      "    infer.propagate(raise_errors=raise_errors)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typeinfer.py\", line 978, in propagate\n",
      "    errors = self.constraints.propagate(self)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typeinfer.py\", line 149, in propagate\n",
      "    constraint(typeinfer)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typeinfer.py\", line 487, in __call__\n",
      "    self.resolve(typeinfer, typevars, fnty)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typeinfer.py\", line 507, in resolve\n",
      "    sig = typeinfer.resolve_call(fnty, pos_args, kw_args)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typeinfer.py\", line 1451, in resolve_call\n",
      "    return self.context.resolve_function_type(fnty, pos_args, kw_args)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typing/context.py\", line 197, in resolve_function_type\n",
      "    res = self._resolve_user_function_type(func, args, kws)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typing/context.py\", line 249, in _resolve_user_function_type\n",
      "    return func.get_call_type(self, args, kws)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/types/functions.py\", line 277, in get_call_type\n",
      "    template, pysig, args, kws = self.dispatcher.get_call_template(args, kws)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/dispatcher.py\", line 301, in get_call_template\n",
      "    self.compile(tuple(args))\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler_lock.py\", line 32, in _acquire_compile_lock\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/dispatcher.py\", line 794, in compile\n",
      "    cres = self._compiler.compile(args, return_type)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/dispatcher.py\", line 77, in compile\n",
      "    status, retval = self._compile_cached(args, return_type)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/dispatcher.py\", line 91, in _compile_cached\n",
      "    retval = self._compile_core(args, return_type)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/dispatcher.py\", line 109, in _compile_core\n",
      "    pipeline_class=self.pipeline_class)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler.py\", line 568, in compile_extra\n",
      "    return pipeline.compile_extra(func)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler.py\", line 339, in compile_extra\n",
      "    return self._compile_bytecode()\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler.py\", line 401, in _compile_bytecode\n",
      "    return self._compile_core()\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler.py\", line 372, in _compile_core\n",
      "    pm.run(self.state)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler_machinery.py\", line 332, in run\n",
      "    self._runPass(idx, pass_inst, state)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler_lock.py\", line 32, in _acquire_compile_lock\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler_machinery.py\", line 291, in _runPass\n",
      "    mutated |= check(pss.run_pass, internal_state)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/compiler_machinery.py\", line 264, in check\n",
      "    mangled = func(compiler_state)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/typed_passes.py\", line 226, in run_pass\n",
      "    rewrites.rewrite_registry.apply('after-inference', state)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/rewrites/registry.py\", line 68, in apply\n",
      "    state.calltypes)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/inline_closurecall.py\", line 1039, in match\n",
      "    self.typingctx, typemap, calltypes)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/ir_utils.py\", line 1450, in guard\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/numba/core/inline_closurecall.py\", line 1190, in _inline_const_arraycall\n",
      "    for inst in block.body:\n",
      "KeyboardInterrupt\n",
      "/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "! mkdir preprocessed_datasets\n",
    "! python magenta/scripts/convert_dir_to_note_sequences.py --input_dir=./groove/train --output_file=./preprocessed_datasets/groove_train.tfrecord --recursive=True --log=INFO\n",
    "! python magenta/scripts/convert_dir_to_note_sequences.py --input_dir=./groove/val --output_file=./preprocessed_datasets/groove_validation.tfrecord --recursive=True --log=INFO\n",
    "! python magenta/scripts/convert_dir_to_note_sequences.py --input_dir=./groove/test --output_file=./preprocessed_datasets/groove_test.tfrecord --recursive=True --log=INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MusicVAE model\n",
    "![MusicVae](attachment:musicvae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make MusicVAE config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/Users/minji/miniconda3/envs/magenta/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import magenta\n",
    "\n",
    "import collections\n",
    "\n",
    "from magenta.models.music_vae.configs import Config, HParams, CONFIG_MAP\n",
    "\n",
    "from magenta.common import merge_hparams\n",
    "from magenta.contrib import training as contrib_training\n",
    "from magenta.models.music_vae import data\n",
    "from magenta.models.music_vae import data_hierarchical\n",
    "from magenta.models.music_vae import lstm_models\n",
    "from magenta.models.music_vae.base_model import MusicVAE\n",
    "\n",
    "import note_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters in paper\n",
    "\n",
    "NUM_FEATURES = 2 ** 9\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "ENCODER_HIDDEN_SIZE = 2048\n",
    "LATENT_DIM = 512\n",
    "NUM_SEGMENTS = 4\n",
    "CONDUCTOR_HIDDEN_SIZE = 1024\n",
    "CONDUCTOR_OUTPUT_SIZE = 512\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DECODER_HIDDEN_SIZE = 1024\n",
    "DECODER_OUTPUT_SIZE = NUM_FEATURES\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "NUM_BARS = 4\n",
    "MAX_SEQ_LENGTH = NUM_BARS * NUM_SEGMENTS\n",
    "BATCH_SIZE = 512\n",
    "MAX_BETA = 0.2 # lower beta means better reconstrucction\n",
    "FREE_BITS = 48\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MAP['musicvae_groove_4bar'] = Config(\n",
    "    model=MusicVAE(\n",
    "        lstm_models.BidirectionalLstmEncoder(),\n",
    "        lstm_models.HierarchicalLstmDecoder(\n",
    "            lstm_models.CategoricalLstmDecoder(),\n",
    "            level_lengths=[4, 4])\n",
    "        ),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_seq_len=MAX_SEQ_LENGTH,  \n",
    "            z_size=LATENT_DIM,\n",
    "            enc_rnn_size=[ENCODER_HIDDEN_SIZE],\n",
    "            dec_rnn_size=[DECODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE],\n",
    "            max_beta=MAX_BETA,\n",
    "            free_bits=FREE_BITS,\n",
    "            dropout_keep_prob=0.3,\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.DrumsConverter(\n",
    "        max_bars=100,  # Truncate long drum sequences before slicing.\n",
    "        slice_bars=4,\n",
    "        steps_per_quarter=4,\n",
    "        roll_input=True),\n",
    "    train_examples_path=\"./preprocessed_datasets/groove_train.tfrecord\",  # the train tfrecord file path\n",
    "    eval_examples_path=\"./preprocessed_datasets/groove_validation.tfrecord\" # the eval tfrecord file path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MusicVAE\n",
    "\n",
    "We use the script(https://github.com/magenta/magenta/blob/main/magenta/models/music_vae/music_vae_train.py) to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "absl.app:\n",
      "  --[no]only_check_args: Set to true to validate args and exit.\n",
      "    (default: 'false')\n",
      "  --[no]pdb: Alias for --pdb_post_mortem.\n",
      "    (default: 'false')\n",
      "  --[no]pdb_post_mortem: Set to true to handle uncaught exceptions with PDB post\n",
      "    mortem.\n",
      "    (default: 'false')\n",
      "  --profile_file: Dump profile information to a file (for python -m pstats).\n",
      "    Implies --run_with_profiling.\n",
      "  --[no]run_with_pdb: Set to true for PDB debug mode\n",
      "    (default: 'false')\n",
      "  --[no]run_with_profiling: Set to true for profiling the script. Execution will\n",
      "    be slower, and the output format might change over time.\n",
      "    (default: 'false')\n",
      "  --[no]use_cprofile_for_profiling: Use cProfile instead of the profile module\n",
      "    for profiling. This has no effect unless --run_with_profiling is set.\n",
      "    (default: 'true')\n",
      "\n",
      "absl.logging:\n",
      "  --[no]alsologtostderr: also log to stderr?\n",
      "    (default: 'false')\n",
      "  --log_dir: directory to write logfiles into\n",
      "    (default: '')\n",
      "  --logger_levels: Specify log level of loggers. The format is a CSV list of\n",
      "    `name:level`. Where `name` is the logger name used with\n",
      "    `logging.getLogger()`, and `level` is a level name  (INFO, DEBUG, etc). e.g.\n",
      "    `myapp.foo:INFO,other.logger:DEBUG`\n",
      "    (default: '')\n",
      "  --[no]logtostderr: Should only log to stderr?\n",
      "    (default: 'false')\n",
      "  --[no]showprefixforinfo: If False, do not prepend prefix to info messages when\n",
      "    it's logged to stderr, --verbosity is set to INFO level, and python logging\n",
      "    is used.\n",
      "    (default: 'true')\n",
      "  --stderrthreshold: log messages at this level, or more severe, to stderr in\n",
      "    addition to the logfile.  Possible values are 'debug', 'info', 'warning',\n",
      "    'error', and 'fatal'.  Obsoletes --alsologtostderr. Using --alsologtostderr\n",
      "    cancels the effect of this flag. Please also note that this flag is subject\n",
      "    to --verbosity and requires logfile not be stderr.\n",
      "    (default: 'fatal')\n",
      "  -v,--verbosity: Logging verbosity level. Messages logged at this level or\n",
      "    lower will be included. Set to 1 for debug logging. If the flag was not set\n",
      "    or supplied, the value will be changed from the default of -1 (warning) to 0\n",
      "    (info) after flags are parsed.\n",
      "    (default: '-1')\n",
      "    (an integer)\n",
      "\n",
      "absl.testing.absltest:\n",
      "  --test_random_seed: Random seed for testing. Some test frameworks may change\n",
      "    the default value of this flag between runs, so it is not appropriate for\n",
      "    seeding probabilistic tests.\n",
      "    (default: '301')\n",
      "    (an integer)\n",
      "  --test_randomize_ordering_seed: If positive, use this as a seed to randomize\n",
      "    the execution order for test cases. If \"random\", pick a random seed to use.\n",
      "    If 0 or not set, do not randomize test case execution order. This flag also\n",
      "    overrides the TEST_RANDOMIZE_ORDERING_SEED environment variable.\n",
      "    (default: '')\n",
      "  --test_srcdir: Root of directory tree where source files live\n",
      "    (default: '')\n",
      "  --test_tmpdir: Directory for temporary testing files\n",
      "    (default: '/var/folders/0f/pnfljr415b537psr3pxtb3xc0000gn/T/absl_testing')\n",
      "  --xml_output_file: File to store XML test results\n",
      "    (default: '')\n",
      "\n",
      "tensorflow.python.ops.parallel_for.pfor:\n",
      "  --[no]op_conversion_fallback_to_while_loop: DEPRECATED: Flag is ignored.\n",
      "    (default: 'true')\n",
      "\n",
      "tensorflow.python.tpu.client.client:\n",
      "  --[no]hbm_oom_exit: Exit the script when the TPU HBM is OOM.\n",
      "    (default: 'true')\n",
      "  --[no]runtime_oom_exit: Exit the script when the TPU runtime is OOM.\n",
      "    (default: 'true')\n",
      "\n",
      "tensorflow_datasets.core.community.register_path:\n",
      "  --[no]tfds_debug_list_dir: Debug the catalog generation\n",
      "    (default: 'false')\n",
      "\n",
      "tensorflow_datasets.text.wikipedia:\n",
      "  --[no]wikipedia_auto_select_flume_mode: If True, will automatically determine\n",
      "    whether to run flume on borg or locally based on the dump size for each\n",
      "    language.\n",
      "    (default: 'true')\n",
      "\n",
      "absl.flags:\n",
      "  --flagfile: Insert flag definitions from the given file into the command line.\n",
      "    (default: '')\n",
      "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
      "    the command line even if the program does not define a flag with that name.\n",
      "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
      "    format.\n",
      "    (default: '')\n"
     ]
    }
   ],
   "source": [
    "from magenta.models.music_vae.music_vae_train import main, flags, FLAGS, train, evaluate\n",
    "from magenta.models.music_vae import configs\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set arguments for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from argparse import ArgumentParser\n",
    "\n",
    "# parser = ArgumentParser(description=\"Argument for training\")\n",
    "# parser.add_argument(\"examples_path\", type=str, default=\"./preprocessed_datasets/groove.tfrecord\", help=\"Path to a TFRecord file of NoteSequence examples. Overrides the config.\")\n",
    "# parser.add_argument(\"tfds_name\", type=str, default='', help=\"TensorFlow Datasets dataset name to use. Overrides the config.\")\n",
    "# parser.add_argument(\"run_dir\", type=str, default=\"./outputs/\", help=\"Path where checkpoints and summary events will be located during training and evaluation. Separate subdirectories 'train' and 'eval' will be created within this directory.\")\n",
    "# parser.add_argument(\"num_steps\", type=int, default=50000, help=\"Number of training steps or `None` for infinite.\")\n",
    "# parser.add_argument(\"eval_num_batches\", type=int, default=2000, help=\"Number of batches to use during evaluation or 'None' for all batches in the data source.\")\n",
    "# parser.add_argument(\"checkpoints_to_keep\", type=int, default=25, help=\"Maximum number of checkpoints to keep in 'train' mode or 0 for infinite.\")\n",
    "# parser.add_argument(\"mode\", type=str, default=\"train\", help=\"train or eval\")\n",
    "# parser.add_argument(\"log\", type=str, default=\"INFO\", help=\"DEBUG, INFO, WARN, ERROR, or FATAL.\")\n",
    "\n",
    "args = {\n",
    "    \"master\": '',\n",
    "    \"examples_path\": \"\", # we have already set examples path in config\n",
    "    \"tfds_name\": \"\",\n",
    "    \"run_dir\": \"./outputs/\",\n",
    "    \"num_steps\": 500000,\n",
    "    \"eval_num_batches\": 2000,\n",
    "    \"checkpoints_to_keep\": 25,\n",
    "    \"keep_checkpoint_every_n_hours\": 1,\n",
    "    \"mode\": \"train\",\n",
    "    \"log\": \"INFO\",\n",
    "    \"hparams\": '',\n",
    "    \"cache_dataset\": True,\n",
    "    \"task\": 0,\n",
    "    \"num_ps_tasks\": 0,\n",
    "    'num_sync_workers': 0,\n",
    "    'eval_dir_suffix': '',\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(args[\"log\"])\n",
    "\n",
    "def run(config_map, args, tf_file_reader=tf.data.TFRecordDataset, file_reader=tf.python_io.tf_record_iterator):\n",
    "    if not args[\"run_dir\"]:\n",
    "        raise ValueError('Invalid run directory: %s' % args[\"run_dir\"])\n",
    "    run_dir = os.path.expanduser(args[\"run_dir\"])\n",
    "    train_dir = os.path.join(run_dir, 'train')\n",
    "    \n",
    "    if args[\"mode\"] not in ['train', 'eval']:\n",
    "      raise ValueError('Invalid mode: %s' % args[\"mode\"])\n",
    "\n",
    "    config = config_map\n",
    "    if args[\"hparams\"]:\n",
    "      config.hparams.parse(args[\"hparams\"])\n",
    "    config_update_map = {}\n",
    "    if args[\"examples_path\"]:\n",
    "      config_update_map['%s_examples_path' % args[\"mode\"]] = os.path.expanduser(args[\"examples_path\"])\n",
    "    if args[\"tfds_name\"]:\n",
    "      if args[\"examples_path\"]:\n",
    "        raise ValueError(\n",
    "            'At most one of --examples_path and --tfds_name can be set.')\n",
    "      config_update_map['tfds_name'] = args[\"tfds_name\"]\n",
    "      config_update_map['eval_examples_path'] = None\n",
    "      config_update_map['train_examples_path'] = None\n",
    "    config = configs.update_config(config, config_update_map)\n",
    "\n",
    "    if args[\"mode\"] == 'train':\n",
    "      is_training = True\n",
    "    elif args[\"mode\"] == 'eval':\n",
    "      is_training = False\n",
    "    else:\n",
    "      raise ValueError('Invalid mode: {}'.format(args[\"mode\"]))\n",
    "\n",
    "    def dataset_fn():\n",
    "      return data.get_dataset(\n",
    "          config,\n",
    "          tf_file_reader=tf_file_reader,\n",
    "          is_training=is_training,\n",
    "          cache_dataset=args[\"cache_dataset\"])\n",
    "\n",
    "    if is_training:\n",
    "      train(\n",
    "          train_dir,\n",
    "          config=config,\n",
    "          dataset_fn=dataset_fn,\n",
    "          checkpoints_to_keep=args[\"checkpoints_to_keep\"],\n",
    "          keep_checkpoint_every_n_hours=args[\"keep_checkpoint_every_n_hours\"],\n",
    "          num_steps=args[\"num_steps\"],\n",
    "          master=args[\"master\"],\n",
    "          num_sync_workers=args[\"num_sync_workers\"],\n",
    "          num_ps_tasks=args[\"num_ps_tasks\"],\n",
    "          task=args[\"task\"])\n",
    "    else:\n",
    "      num_batches = args[\"eval_num_batches\"] or data.count_examples(\n",
    "          config.eval_examples_path,\n",
    "          config.tfds_name,\n",
    "          config.data_converter,\n",
    "          file_reader) // config.hparams.batch_size\n",
    "      eval_dir = os.path.join(run_dir, 'eval' + args[\"eval_dir_suffix\"])\n",
    "      evaluate(\n",
    "          train_dir,\n",
    "          eval_dir,\n",
    "          config=config,\n",
    "          dataset_fn=dataset_fn,\n",
    "          num_batches=num_batches,\n",
    "          master=args[\"master\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder, and hparams:\n",
      "{'max_seq_len': 64, 'z_size': 512, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 512, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [1024, 1024], 'enc_rnn_size': [2048], 'dropout_keep_prob': 0.3, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [2048]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [1024, 1024]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:Reading examples from file: ./preprocessed_datasets/groove_train.tfrecord\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0f/pnfljr415b537psr3pxtb3xc0000gn/T/ipykernel_45239/1878518343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG_MAP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'musicvae_groove_4bar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/0f/pnfljr415b537psr3pxtb3xc0000gn/T/ipykernel_45239/3304947540.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config_map, args, tf_file_reader, file_reader)\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mnum_sync_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_sync_workers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mnum_ps_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_ps_tasks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m           task=args[\"task\"])\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       num_batches = args[\"eval_num_batches\"] or data.count_examples(\n",
      "\u001b[0;32m~/Work/personal/music_vae/magenta/magenta/models/music_vae/music_vae_train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dir, config, dataset_fn, checkpoints_to_keep, keep_checkpoint_every_n_hours, num_steps, master, num_sync_workers, num_ps_tasks, task)\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m           \u001b[0msave_checkpoint_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m           \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m           is_chief=is_chief)\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tf_slim/training/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_op, logdir, master, is_chief, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, config, max_wait_secs, run_metadata)\u001b[0m\n\u001b[1;32m    546\u001b[0m       \u001b[0msave_summaries_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_summaries_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       max_wait_secs=max_wait_secs) as session:\n\u001b[0m\u001b[1;32m    549\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[0;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir, save_graph_def)\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0msession_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m       stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    755\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \"\"\"\n\u001b[1;32m   1262\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         logging.info(\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    908\u001b[0m       \u001b[0;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m       \u001b[0;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    321\u001b[0m                            \"init_fn or local_init_op was given\")\n\u001b[1;32m    322\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1371\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1361\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m~/miniconda3/envs/magenta/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1398\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(CONFIG_MAP['musicvae_groove_4bar'], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples\n",
    "Magenta also provides scripts to generate samples from trained models  \n",
    "scripts: magenta/magenta/models/music_vae/music_vae_generate.py\n",
    "\n",
    "pretrained model link: https://drive.google.com/file/d/1ALoQpdyUI5oHJCqe92Cb_oimv1CXP3cN/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "import sys\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your checkpoint path. It will be .tar file including .index and .data\n",
    "checkpoint_path = Path('./outputs/model.ckpt-50000.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"run_dir\": None,\n",
    "    \"checkpoint_file\": checkpoint_path,\n",
    "    \"output_dir\": \"./outputs/generation_samples/\",\n",
    "    \"mode\": \"sample\",\n",
    "    \"input_midi_1\": None,\n",
    "    \"input_midi_2\": None,\n",
    "    \"num_outputs\": 5,\n",
    "    \"max_batch_size\": 8,\n",
    "    \"temperature\": 0.5,\n",
    "    \"log\": \"INFO\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior()\n",
    "logging = tf.logging\n",
    "logging.set_verbosity(generation_args[\"log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_run(config_map, generation_args):\n",
    "  \"\"\"Load model params, save config file and start trainer.\n",
    "  Args:\n",
    "    config_map: Dictionary mapping configuration name to Config object.\n",
    "  Raises:\n",
    "    ValueError: if required flags are missing or invalid.\n",
    "  \"\"\"\n",
    "  date_and_time = time.strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "  if generation_args[\"run_dir\"] is None == generation_args[\"checkpoint_file\"] is None:\n",
    "    raise ValueError(\n",
    "        'Exactly one of `--run_dir` or `--checkpoint_file` must be specified.')\n",
    "  if generation_args[\"output_dir\"] is None:\n",
    "    raise ValueError('`--output_dir` is required.')\n",
    "  tf.gfile.MakeDirs(generation_args[\"output_dir\"])\n",
    "  if generation_args[\"mode\"] != 'sample' and generation_args[\"mode\"] != 'interpolate':\n",
    "    raise ValueError('Invalid value for `--mode`: %s' % generation_args[\"mode\"])\n",
    "\n",
    "  config = config_map\n",
    "  config.data_converter.max_tensors_per_item = None\n",
    "\n",
    "  if generation_args[\"mode\"] == 'interpolate':\n",
    "    if generation_args[\"input_midi_1\"] is None or generation_args[\"input_midi_2\"] is None:\n",
    "      raise ValueError(\n",
    "          '`--input_midi_1` and `--input_midi_2` must be specified in '\n",
    "          '`interpolate` mode.')\n",
    "    input_midi_1 = os.path.expanduser(generation_args[\"input_midi_1\"])\n",
    "    input_midi_2 = os.path.expanduser(generation_args[\"input_midi_2\"])\n",
    "    if not os.path.exists(input_midi_1):\n",
    "      raise ValueError('Input MIDI 1 not found: %s' % generation_args[\"input_midi_1\"])\n",
    "    if not os.path.exists(input_midi_2):\n",
    "      raise ValueError('Input MIDI 2 not found: %s' % generation_args[\"input_midi_2\"])\n",
    "    input_1 = note_seq.midi_file_to_note_sequence(input_midi_1)\n",
    "    input_2 = note_seq.midi_file_to_note_sequence(input_midi_2)\n",
    "\n",
    "    def _check_extract_examples(input_ns, path, input_number):\n",
    "      \"\"\"Make sure each input returns exactly one example from the converter.\"\"\"\n",
    "      tensors = config.data_converter.to_tensors(input_ns).outputs\n",
    "      if not tensors:\n",
    "        print(\n",
    "            'MusicVAE configs have very specific input requirements. Could not '\n",
    "            'extract any valid inputs from `%s`. Try another MIDI file.' % path)\n",
    "        sys.exit()\n",
    "      elif len(tensors) > 1:\n",
    "        basename = os.path.join(\n",
    "            generation_args[\"output_dir\"],\n",
    "            '%s_input%d-extractions_%s-*-of-%03d.mid' %\n",
    "            (\"musicvae_groove_4bar\", input_number, date_and_time, len(tensors)))\n",
    "        for i, ns in enumerate(config.data_converter.from_tensors(tensors)):\n",
    "          note_seq.sequence_proto_to_midi_file(\n",
    "              ns, basename.replace('*', '%03d' % i))\n",
    "        print(\n",
    "            '%d valid inputs extracted from `%s`. Outputting these potential '\n",
    "            'inputs as `%s`. Call script again with one of these instead.' %\n",
    "            (len(tensors), path, basename))\n",
    "        sys.exit()\n",
    "    logging.info(\n",
    "        'Attempting to extract examples from input MIDIs using config `%s`...',\n",
    "        \"musicvae_groove_4bar\")\n",
    "    _check_extract_examples(input_1, generation_args[\"input_midi_1\"], 1)\n",
    "    _check_extract_examples(input_2, generation_args[\"input_midi_2\"], 2)\n",
    "\n",
    "  logging.info('Loading model...')\n",
    "  if generation_args[\"run_dir\"]:\n",
    "    checkpoint_dir_or_path = os.path.expanduser(\n",
    "        os.path.join(generation_args[\"run_dir\"], 'train'))\n",
    "  else:\n",
    "    checkpoint_dir_or_path = os.path.expanduser(generation_args[\"checkpoint_file\"])\n",
    "    print(checkpoint_dir_or_path)\n",
    "  model = TrainedModel(\n",
    "      config, batch_size=min(generation_args[\"max_batch_size\"], generation_args[\"num_outputs\"]),\n",
    "      checkpoint_dir_or_path=checkpoint_dir_or_path)\n",
    "\n",
    "  if generation_args[\"mode\"] == 'interpolate':\n",
    "    logging.info('Interpolating...')\n",
    "    _, mu, _ = model.encode([input_1, input_2])\n",
    "    z = np.array([\n",
    "        _slerp(mu[0], mu[1], t) for t in np.linspace(0, 1, generation_args[\"num_outputs\"])])\n",
    "    results = model.decode(\n",
    "        length=config.hparams.max_seq_len,\n",
    "        z=z,\n",
    "        temperature=generation_args[\"temperature\"])\n",
    "  elif generation_args[\"mode\"] == 'sample':\n",
    "    logging.info('Sampling...')\n",
    "    results = model.sample(\n",
    "        n=generation_args[\"num_outputs\"],\n",
    "        length=config.hparams.max_seq_len,\n",
    "        temperature=generation_args[\"temperature\"])\n",
    "\n",
    "  basename = os.path.join(\n",
    "      generation_args[\"output_dir\"],\n",
    "      '%s_%s_%s-*-of-%03d.mid' %\n",
    "      (\"musicvae_groove_4bar\", generation_args[\"mode\"], date_and_time, generation_args[\"num_outputs\"]))\n",
    "  logging.info('Outputting %d files as `%s`...', generation_args[\"num_outputs\"], basename)\n",
    "  for i, ns in enumerate(results):\n",
    "    note_seq.sequence_proto_to_midi_file(ns, basename.replace('*', '%03d' % i))\n",
    "\n",
    "  logging.info('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model...\n",
      "outputs/model.ckpt-50000.tar\n",
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, HierarchicalLstmDecoder, and hparams:\n",
      "{'max_seq_len': 16, 'z_size': 512, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 5, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [1024, 1024], 'enc_rnn_size': [2048], 'dropout_keep_prob': 0.3, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [2048]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Hierarchical Decoder:\n",
      "  input length: 16\n",
      "  level output lengths: [4, 4]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [1024, 1024]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:Unbundling checkpoint.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/0f/pnfljr415b537psr3pxtb3xc0000gn/T/tmpfcyah7dk/model.ckpt-50000/model.ckpt-50000\n",
      "INFO:tensorflow:Sampling...\n",
      "INFO:tensorflow:Outputting 5 files as `./outputs/generation_samples/musicvae_groove_4bar_sample_2022-10-05_094302-*-of-005.mid`...\n",
      "INFO:tensorflow:Done.\n"
     ]
    }
   ],
   "source": [
    "generation_run(CONFIG_MAP[\"musicvae_groove_4bar\"], generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('magenta')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0746120117e608c640370de5789262e88f3f302f4d84eea41e4e29e486cf0a25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
